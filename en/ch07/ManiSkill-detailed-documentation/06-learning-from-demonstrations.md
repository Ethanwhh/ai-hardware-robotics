# Learning from Demonstrations

ManiSkill supports various learning/imitation learning methods from demonstrations through a unified API, and provides multiple ready-to-use, tested baselines for use/comparison. The following pages show how to [set up datasets/environments for learning from demonstrations](https://7mlcen.aitianhu6.top/c/setup.md) and how to use [baselines](https://7mlcen.aitianhu6.top/c/baselines.md). All baseline results are published to our [public wandb page](https://wandb.ai/stonet2000/ManiSkill). On this page, you can filter by the algorithm used, environment type, etc. We are still conducting all experiments, so not all results have been uploaded yet.

# Setup

This page documents key considerations for using demonstration data in ManiSkill environments, including:

- How to [download and replay trajectories to standard datasets](https://7mlcen.aitianhu6.top/c/67dd73ff-bfe0-800d-bd34-d075378f1c3a#downloading-and-replaying-trajectories--standard-datasets), which are used for benchmarking state-based and vision-based imitation learning
- How to [fairly and correctly evaluate trained models](https://7mlcen.aitianhu6.top/c/67dd73ff-bfe0-800d-bd34-d075378f1c3a#evaluation)
- Some common [pitfalls to avoid](https://7mlcen.aitianhu6.top/c/67dd73ff-bfe0-800d-bd34-d075378f1c3a#common-pitfalls-to-avoid)

## Downloading and Replaying Trajectories / Standard Datasets

By default, ManiSkill's demonstration data is stored in a highly simplified/compressed format that does not contain any observation data for fast download and small file size. Run the following command to download the original minimal demonstration data:

```bash
python -m mani_skill.utils.download_demo "PickCube-v1"
```

To ensure everyone uses the same preprocessing/replay dataset, be sure to run the following script: [ManiSkill/scripts/data_generation/replay_for_il_baselines.sh at main · haosulab/ManiSkill · GitHub](https://github.com/haosulab/ManiSkill/blob/main/scripts/data_generation/replay_for_il_baselines.sh). Note that some scripts use GPU simulation (via the `-b physx_cuda` flag) for replay, which may require more GPU memory than you have available. If needed, you can reduce the number of parallel replay environments by setting the `--num-procs` parameter to a smaller value.

This script fixes the settings for trajectory replay to generate observation data and set the desired action space/controller for all benchmark tasks. All benchmark training run results detailed in the [Wandb project](https://wandb.ai/stonet2000/ManiSkill) use data generated by the above script for replay.

If you need more advanced trajectory replay use cases (e.g., generating point clouds, changing controller modes), please refer to the [Trajectory Replay Documentation](https://7mlcen.aitianhu6.top/datasets/replay.md). If you wish to generate the original dataset locally, we have saved all scripts used for dataset generation in the [data_generation](https://github.com/haosulab/ManiSkill/tree/main/scripts/data_generation) folder.

## Evaluation

Due to the existence of multiple types of environments, algorithms, and evaluation methods, the following describes a consistent and standardized way to fairly evaluate various demonstration learning strategies in ManiSkill. In short, to ensure fair evaluation, the following settings are required:

- Disable partial resets, the environment will not reset on success/failure/termination (`ignore_terminations=True`). Instead, record multiple success/failure metrics.
- All parallel environments will be reconfigured on reset (`reconfiguration_freq=1`), which will randomize the geometry of objects in the task (if the task includes object randomization).

The following is a code example for fairly evaluating strategies in ManiSkill and recording standard metrics. We provide options for CPU and GPU vectorized environments separately, because depending on the simulation backend used to collect your demonstration data, you may want to evaluate your strategy on the same backend.

### Evaluation Code Example with GPU Vectorized Environment

```python
import gymnasium as gym
from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv
env_id = "PushCube-v1"
num_eval_envs = 64
env_kwargs = dict(obs_mode="state")  # Modify env_kwargs as needed
eval_envs = gym.make(env_id, num_envs=num_eval_envs, reconfiguration_freq=1, **env_kwargs)
# Add other required wrappers here
eval_envs = ManiSkillVectorEnv(eval_envs, ignore_terminations=True, record_metrics=True)

# Evaluation loop, only record metrics for complete episodes
batch_obs, _ = eval_envs.reset(seed=0)
eval_metrics = defaultdict(list)
for _ in range(400):
    actions = eval_envs.action_space.sample()  # Replace this line with your policy actions
    batch_obs, batch_rews, batch_terminated, batch_truncated, batch_info = eval_envs.step(actions)
    # Note that due to no partial resets, all environments' truncated are True at the same time
    if batch_truncated.any():
        for k, v in batch_info["final_info"]["episode"].items():
            eval_metrics[k].append(v.float())
for k in eval_metrics.keys():
    print(f"{k}_mean: {torch.mean(torch.stack(eval_metrics[k])).item()}")
```

### Evaluation Code Example with CPU Vectorized Environment

```python
import gymnasium as gym
from mani_skill.utils.wrappers import CPUGymWrapper
env_id = "PickCube-v1"
num_eval_envs = 8
env_kwargs = dict(obs_mode="state")  # Modify env_kwargs as needed

def cpu_make_env(env_id, env_kwargs=dict()):
    def thunk():
        env = gym.make(env_id, reconfiguration_freq=1, **env_kwargs)
        env = CPUGymWrapper(env, ignore_terminations=True, record_metrics=True)
        # Add other required wrappers here
        return env
    return thunk

vector_cls = gym.vector.SyncVectorEnv if num_eval_envs == 1 else lambda x: gym.vector.AsyncVectorEnv(x, context="forkserver")
eval_envs = vector_cls([cpu_make_env(env_id, env_kwargs) for _ in range(num_eval_envs)])

# Evaluation loop, only record metrics for complete episodes
batch_obs, _ = eval_envs.reset(seed=0)
eval_metrics = defaultdict(list)
for _ in range(400):
    actions = eval_envs.action_space.sample()  # Replace this line with your policy actions
    batch_obs, batch_rews, batch_terminated, batch_truncated, batch_info = eval_envs.step(actions)
    # Note that due to no partial resets, all environments' truncated are True at the same time
    if batch_truncated.any():
        for final_info in batch_info["final_info"]:
            for k, v in final_info["episode"].items():
                eval_metrics[k].append(v)
for k in eval_metrics.keys():
    print(f"{k}_mean: {np.mean(eval_metrics[k])}")
```

The following are the recorded and explained metrics:

- **success_once**: Whether the task was successful at any point during the episode.
- **success_at_end**: Whether the task was successful at the last step of the episode.
- **fail_once / fail_at_end**: Similar to the above two, but for failure cases. Note that not all tasks have success/failure criteria.
- **return**: The sum of accumulated rewards throughout the episode.

Generally, for demonstration learning, the only important metric is **"success_once"**, which is what is typically reported in ManiSkill-related research and work.

## Common Pitfalls to Avoid

In general, if demonstration data was collected in, for example, a PhysX CPU simulation environment, you need to ensure that any policy trained on that data is evaluated on the same simulation backend. This is especially important for highly precise tasks (e.g., PushT), where even 1e-3 errors can lead to different results. This is why all demonstration data replayed through our trajectory replay tool will have the simulation backend used marked on the trajectory filename.

The source of your demonstration data will largely affect training performance. Classic behavior cloning methods can better imitate demonstration data generated by neural network/RL policies, but struggle with multimodal demonstration data (e.g., data generated by human teleoperation or motion planning). Methods like Diffusion Policy (DP) were developed to address this issue. If you're unsure, all official ManiSkill datasets will detail how the data was collected and what type of data it is in the trajectory metadata JSON file.