# Reinforcement Learning

ManiSkill supports various reinforcement learning methods through a unified API and provides multiple ready-to-use, tested baselines for use/comparison. The following pages show how to set up reinforcement learning environments and how to use RL baselines. All baseline results are published to our [public wandb page](https://wandb.ai/stonet2000/ManiSkill). On this page, you can filter by the algorithm used, environment type, etc. We are still conducting all experiments, so not all results have been uploaded yet.

# Setup

This page documents key considerations for setting up ManiSkill environments for reinforcement learning, including:

- How to convert ManiSkill environments to gymnasium API compatible environments, including [single](#gym-environment-api) and [vectorized](#gym-vectorized-environment-api) APIs.
- How to [**properly** and fairly evaluate RL policies](#evaluation)
- [Useful wrappers](#useful-wrappers)

ManiSkill environments are created by gymnasium's `make` function. By default, the result is a "batch" environment where every input and output is batched. Note that this is not the standard gymnasium API. If you want the standard gymnasium environment / vectorized environment API, please see the next section.

```python
import mani_skill.envs
import gymnasium as gym
N = 4
env = gym.make("PickCube-v1", num_envs=N)
env.action_space # shape (N, D)
env.observation_space # shape (N, ...)
env.reset()
obs, rew, terminated, truncated, info = env.step(env.action_space.sample())
# obs (N, ...), rew (N, ), terminated (N, ), truncated (N, )
```

## Gym Environment API

If you want to use CPU simulation / single environment, you can apply `CPUGymWrapper`, which essentially unbatches everything and converts everything to numpy, so the environment behaves like a normal gym environment. For details on the gym environment's API, please refer to [its documentation](https://gymnasium.farama.org/api/env/).

```python
import mani_skill.envs
import gymnasium as gym
from mani_skill.utils.wrappers.gymnasium import CPUGymWrapper
N = 1
env = gym.make("PickCube-v1", num_envs=N)
env = CPUGymWrapper(env)
env.action_space # shape (D, )
env.observation_space # shape (...)
env.reset()
obs, rew, terminated, truncated, info = env.step(env.action_space.sample())
# obs (...), rew (float), terminated (bool), truncated (bool)
```

## Gym Vectorized Environment API

We have also adopted the gymnasium `VectorEnv` (also known as `AsyncVectorEnv`) interface, which you can achieve with a single wrapper to allow your algorithms assuming the `VectorEnv` interface to work seamlessly. For details on the vectorized gym environment's API, please refer to [its documentation](https://gymnasium.farama.org/api/vector/)

```python
import mani_skill.envs
import gymnasium as gym
from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv
N = 4
env = gym.make("PickCube-v1", num_envs=N)
env = ManiSkillVectorEnv(env, auto_reset=True, ignore_terminations=False)
env.action_space # shape (N, D)
env.single_action_space # shape (D, )
env.observation_space # shape (N, ...)
env.single_observation_space # shape (...)
env.reset()
obs, rew, terminated, truncated, info = env.step(env.action_space.sample())
# obs (N, ...), rew (N, ), terminated (N, ), truncated (N, )
```

You may also notice there are two additional options when creating the vector environment. The `auto_reset` parameter controls whether an environment is automatically reset when it terminates or truncates. This depends on the algorithm. The `ignore_terminations` parameter controls whether the environment resets when terminated is True. As with gymnasium vector environments, partial resets can occur where some parallel environments reset while others do not.

Note that for efficiency, everything returned by the environment will be batched torch tensors on GPU rather than batched numpy arrays on CPU. This is likely the only difference between ManiSkill vectorized environments and gymnasium vectorized environments that you may need to account for.

## Evaluation

Given the different types of environments, algorithms, and evaluation methods, we describe a consistent and standardized way to fairly evaluate all types of policies in ManiSkill below. In summary, the following setup is required to ensure fair evaluation:

- Turn off partial resets and do not reset the environment on success/failure/termination (`ignore_terminations=True`). Instead, we record multiple types of success/failure metrics.
- All parallel environments reconfigure on reset (`reconfiguration_freq=1`), which randomizes object geometries if the task has object randomization.

The following code shows how to fairly evaluate policies in ManiSkill and record standard metrics. For GPU vectorized environments, the recommended code to evaluate policies by environment ID is as follows:

```python
import gymnasium as gym
from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv
env_id = "PushCube-v1"
num_eval_envs = 64
env_kwargs = dict(obs_mode="state") # modify your env_kwargs here
eval_envs = gym.make(env_id, num_envs=num_eval_envs, reconfiguration_freq=1, **env_kwargs)
# add any other wrappers here
eval_envs = ManiSkillVectorEnv(eval_envs, ignore_terminations=True, record_metrics=True)

# evaluation loop, which will record metrics for complete episodes only
batch_obs, _ = eval_envs.reset(seed=0)
eval_metrics = defaultdict(list)
for _ in range(400):
    actions = eval_envs.action_space.sample() # replace with your policy action
    batch_obs, batch_rews, batch_terminated, batch_truncated, batch_info = eval_envs.step(actions)
    # note as there are no partial resets, truncated is True for all environments at the same time
    if batch_truncated.any():
        for k, v in batch_info["final_info"]["episode"].items():
            eval_metrics[k].append(v.float())
for k in eval_metrics.keys():
    print(f"{k}_mean: {torch.mean(torch.stack(eval_metrics[k])).item()}")
```

For CPU vectorized environments, the recommended code for evaluation is:

```python
import gymnasium as gym
from mani_skill.utils.wrappers import CPUGymWrapper
env_id = "PickCube-v1"
num_eval_envs = 8
env_kwargs = dict(obs_mode="state") # modify your env_kwargs here

def cpu_make_env(env_id, env_kwargs = dict()):
    def thunk():
        env = gym.make(env_id, reconfiguration_freq=1, **env_kwargs)
        env = CPUGymWrapper(env, ignore_terminations=True, record_metrics=True)
        # add any other wrappers here
        return env
    return thunk
vector_cls = gym.vector.SyncVectorEnv if num_eval_envs == 1 else lambda x : gym.vector.AsyncVectorEnv(x, context="forkserver")
eval_envs = vector_cls([cpu_make_env(env_id, env_kwargs) for _ in range(num_eval_envs)])

# evaluation loop, which will record metrics for complete episodes only
batch_obs, _ = eval_envs.reset(seed=0)
eval_metrics = defaultdict(list)
for _ in range(400):
    actions = eval_envs.action_space.sample() # replace with your policy action
    batch_obs, batch_rews, batch_terminated, batch_truncated, batch_info = eval_envs.step(actions)
    # note as there are no partial resets, truncated is True for all environments at the same time
    if batch_truncated.any():
        for final_info in batch_info["final_info"]:
            for k, v in final_info["episode"].items():
                eval_metrics[k].append(v)
for k in eval_metrics.keys():
    print(f"{k}_mean: {np.mean(eval_metrics[k])}")
```

The following metrics are recorded and explained below:

- `success_once`: Whether the task was successful at any point during the episode.
- `success_at_end`: Whether the task was successful at the last step of the episode.
- `fail_once/fail_at_end`: Similar to the above two metrics, but for failures. Note that not all tasks have success/failure criteria.
- `return`: The total accumulated reward over the episode.

## Useful Wrappers

RL practitioners often use wrappers to modify and enhance environments. These are documented in the wrappers section. Some commonly used ones include:

- RecordEpisode for recording videos/trajectories of rollouts.
- FlattenRGBDObservations for flattening `obs_mode="rgbd"` or `obs_mode="rgb+depth"` observations into a simple dictionary containing only the combined `rgbd` tensor and `state` tensor.

## Common Errors / Caveats

In older environments/benchmarks, people often use `env.render(mode="rgb_array")` or `env.render()` to get image inputs for RL agents. This is incorrect because image observations are returned directly by `env.reset()` and `env.step()`, and `env.render` is only for visualization/video recording in ManiSkill.

For robotic tasks, observations typically consist of state information (like robot joint angles) and image observations (like camera images). All tasks in ManiSkill will specifically remove certain privileged state information from observations when `obs_mode` is not `state` or `state_dict` (like ground truth object poses). Additionally, image observations returned by `env.reset()` and `env.step()` typically come from cameras positioned to provide a good view of the task, making it solvable.

# Baselines

We provide a variety of different baselines for learning from rewards via online reinforcement learning.

As part of these baselines, we've established standardized reinforcement learning benchmarks covering a wide range of difficulties (easy enough to solve for validation but not saturated) and diversity in robotic task types including but not limited to classic control, dexterous manipulation, tabletop manipulation, mobile manipulation, etc.

## Online Reinforcement Learning Baselines

List of implemented and tested online reinforcement learning baselines. Result links will take you to the corresponding wandb page to view results. You can change filters/views in the wandb workspace to see results with other settings (e.g., state-based or RGB-based training). Note that there are also reinforcement learning with demonstrations (offline RL, online imitation learning) baselines

| Baseline | Code | Results | Paper |
|----------|------|---------|-------|
| Proximal Policy Optimization (PPO) | [Link](https://github.com/haosulab/ManiSkill/blob/main/examples/baselines/ppo) | [Link](https://api.wandb.ai/links/stonet2000/k6lz966q) | [Link](http://arxiv.org/abs/1707.06347) |
| Soft Actor-Critic (SAC) | [Link](https://github.com/haosulab/ManiSkill/blob/main/examples/baselines/sac) | WIP | [Link](https://arxiv.org/abs/1801.01290) |
| Temporal Difference Learning for Model Predictive Control (TD-MPC2) | WIP | WIP | [Link](https://arxiv.org/abs/2310.16828) |

## Standard Benchmarks

The standard benchmarks for reinforcement learning in ManiSkill consist of two sets, a small set of 8 tasks and a large set of 50 tasks, both with state-based and vision-based settings. All standard benchmark tasks come with normalized dense reward functions. A recommended small set was created so researchers without extensive computational resources can still reasonably benchmark/compare their work. The large set is still under development and testing.

These tasks cover an extremely broad range of problems in robotics/RL namely: high-dimensional observations/actions, large initial state distributions, articulated object manipulation, generalizable manipulation, mobile manipulation, locomotion, etc.

**Small Set Environment IDs**:

PushCube-v1, PickCube-v1, PegInsertionSide-v1, PushT-v1, HumanoidPlaceAppleInBowl-v1, AnymalC-Reach-v1, OpenCabinetDrawer-v1

## Evaluation

To properly evaluate reinforcement learning policies, please see the evaluation section in the [Reinforcement Learning Setup page](#evaluation) for how this code is set up. All results reported in the links above follow the same evaluation setup.