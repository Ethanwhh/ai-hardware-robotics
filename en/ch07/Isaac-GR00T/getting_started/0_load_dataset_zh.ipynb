{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51b4cec",
   "metadata": {},
   "source": [
    "# Dataset Loading Guide for Inference\n",
    "\n",
    "## LeRobot Format\n",
    "\n",
    "* This tutorial will show how to use our data loader to load data in LeRobot format.\n",
    "* We will use the `robot_sim.PickNPlace` dataset, which has already been converted to LeRobot format, as an example.\n",
    "* To learn how to convert your own dataset, please refer to [LeRobot.md](LeRobot.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.utils.misc import any_describe\n",
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "from gr00t.data.dataset import ModalityConfig\n",
    "from gr00t.data.schema import EmbodimentTag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f69c00",
   "metadata": {},
   "source": [
    "\n",
    "## Loading the Dataset\n",
    "\n",
    "We need to define 3 things to load the dataset:\n",
    "1. The path to the dataset\n",
    "\n",
    "2. `ModalityConfigs` (Modality Configurations)\n",
    "\n",
    "- `ModalityConfigs` define which data modalities (such as video, state, actions, language) will be used for downstream tasks (like model training or inference).\n",
    "- Each modality specifies which frames to load through delta_indices (e.g., [0] means only the current frame, [-1,0] means the previous frame and current frame)\n",
    "\n",
    "3. `EmbodimentTag` (Embodiment Tag)\n",
    "- `EmbodimentTag` is used to specify the embodiment of the dataset. A list of all embodiment tags can be found in `gr00t.data.embodiment_tags.EmbodimentTag`.\n",
    "- GR00T's architecture has different action heads optimized for specific robot types (embodiments). The `EmbodimentTag` tells the model which action head to use during fine-tuning and/or inference. In our example, since we are using a humanoid arm, we specify `EmbodimentTag.GR1_UNIFIED` to get the best performance from the humanoid-specific action head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import gr00t\n",
    "\n",
    "# REPO_PATH is the path of the pip install gr00t repo and one level up\n",
    "REPO_PATH = os.path.dirname(os.path.dirname(gr00t.__file__))\n",
    "DATA_PATH = os.path.join(REPO_PATH, \"demo_data/robot_sim.PickNPlace\")\n",
    "\n",
    "print(\"Loading dataset... from\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8014c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. modality configs\n",
    "modality_configs = {\n",
    "    \"video\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\"video.ego_view\"],\n",
    "    ),\n",
    "    \"state\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\n",
    "            \"state.left_arm\",\n",
    "            \"state.left_hand\",\n",
    "            \"state.left_leg\",\n",
    "            \"state.neck\",\n",
    "            \"state.right_arm\",\n",
    "            \"state.right_hand\",\n",
    "            \"state.right_leg\",\n",
    "            \"state.waist\",\n",
    "        ],\n",
    "    ),\n",
    "    \"action\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\n",
    "            \"action.left_hand\",\n",
    "            \"action.right_hand\",\n",
    "        ],\n",
    "    ),\n",
    "    \"language\": ModalityConfig(\n",
    "        delta_indices=[0],\n",
    "        modality_keys=[\"annotation.human.action.task_description\", \"annotation.human.validity\"],\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baecc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. gr00t embodiment tag\n",
    "embodiment_tag = EmbodimentTag.GR1\n",
    "\n",
    "# load the dataset\n",
    "dataset = LeRobotSingleDataset(DATA_PATH, modality_configs,  embodiment_tag=embodiment_tag)\n",
    "\n",
    "print('\\n'*2)\n",
    "print(\"=\"*100)\n",
    "print(f\"{' Humanoid Dataset ':=^100}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# print the 7th data point\n",
    "resp = dataset[7]\n",
    "any_describe(resp)\n",
    "print(resp.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a9add6",
   "metadata": {},
   "source": [
    "Display image frames in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12991333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    if i % 10 == 0:\n",
    "        resp = dataset[i]\n",
    "        img = resp[\"video.ego_view\"][0]\n",
    "        images_list.append(img)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.imshow(images_list[i])\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Image {i}\")\n",
    "plt.tight_layout() # adjust the subplots to fit into the figure area.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b5a1c6",
   "metadata": {},
   "source": [
    "## Transform Data\n",
    "\n",
    "We can also apply a series of transformations to the data in our `LeRobotSingleDataset` class. The following shows how to apply transformations to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe728139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.transform.base import ComposedModalityTransform\n",
    "from gr00t.data.transform import VideoToTensor, VideoCrop, VideoResize, VideoColorJitter, VideoToNumpy\n",
    "from gr00t.data.transform.state_action import StateActionToTensor, StateActionTransform\n",
    "from gr00t.data.transform.concat import ConcatTransform\n",
    "\n",
    "\n",
    "video_modality = modality_configs[\"video\"]\n",
    "state_modality = modality_configs[\"state\"]\n",
    "action_modality = modality_configs[\"action\"]\n",
    "\n",
    "# select the transforms you want to apply to the data\n",
    "to_apply_transforms = ComposedModalityTransform(\n",
    "    transforms=[\n",
    "        # video transforms\n",
    "        VideoToTensor(apply_to=video_modality.modality_keys),\n",
    "        VideoCrop(apply_to=video_modality.modality_keys, scale=0.95),\n",
    "        VideoResize(apply_to=video_modality.modality_keys, height=224, width=224, interpolation=\"linear\"),\n",
    "        VideoColorJitter(apply_to=video_modality.modality_keys, brightness=0.3, contrast=0.4, saturation=0.5, hue=0.08),\n",
    "        VideoToNumpy(apply_to=video_modality.modality_keys),\n",
    "\n",
    "        # state transforms\n",
    "        StateActionToTensor(apply_to=state_modality.modality_keys),\n",
    "        StateActionTransform(apply_to=state_modality.modality_keys, normalization_modes={\n",
    "            key: \"min_max\" for key in state_modality.modality_keys\n",
    "        }),\n",
    "\n",
    "        # action transforms\n",
    "        StateActionToTensor(apply_to=action_modality.modality_keys),\n",
    "        StateActionTransform(apply_to=action_modality.modality_keys, normalization_modes={\n",
    "            key: \"min_max\" for key in action_modality.modality_keys\n",
    "        }),\n",
    "\n",
    "        # ConcatTransform\n",
    "        ConcatTransform(\n",
    "            video_concat_order=video_modality.modality_keys,\n",
    "            state_concat_order=state_modality.modality_keys,\n",
    "            action_concat_order=action_modality.modality_keys,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e9e17",
   "metadata": {},
   "source": [
    "\n",
    "Now let's look at how the data changes after applying transformations.\n",
    "\n",
    "For example, state and action data are normalized and concatenated, and video images are cropped, resized, and color-adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LeRobotSingleDataset(\n",
    "    DATA_PATH,\n",
    "    modality_configs,\n",
    "    transforms=to_apply_transforms,\n",
    "    embodiment_tag=embodiment_tag\n",
    ")\n",
    "\n",
    "# print the 7th data point\n",
    "resp = dataset[7]\n",
    "any_describe(resp)\n",
    "print(resp.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
