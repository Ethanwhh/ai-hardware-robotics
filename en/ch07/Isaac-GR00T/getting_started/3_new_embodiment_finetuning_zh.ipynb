{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "487104af",
   "metadata": {},
   "source": [
    "# New Fine-Tuning Tutorial\n",
    "\n",
    "This notebook is a guide to fine-tuning the GR00T-N1 pretrained model on a new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5777eb72",
   "metadata": {},
   "source": [
    "# 1. Lerobot SO100 Fine-Tuning Tutorial\n",
    "\n",
    "GR00T-N1 is open to everyone regardless of robot embodiment. With the low-cost [So100 Lerobot arm](https://github.com/huggingface/lerobot/blob/main/examples/10_use_so100.md) built on Hugging Face, users can fine-tune GR00T-N1 on their own robot using the `new_embodiment` tag.\n",
    "\n",
    "![so100_eval_demo.gif](../media/so100_eval_demo.gif)\n",
    "\n",
    "## Step 1: Dataset\n",
    "\n",
    "Any Lerobot dataset can be used for fine-tuning. In this tutorial we start with the sample dataset [so100_strawberry_grape](https://huggingface.co/spaces/lerobot/visualize_dataset?dataset=youliangtan%2Fso100_strawberry_grape&episode=0).\n",
    "\n",
    "> Note: this embodiment was **not** part of our pre-training mixture.\n",
    "\n",
    "### First, download the dataset\n",
    "\n",
    "```bash\n",
    "huggingface-cli download --repo-type dataset youliangtan/so100_strawberry_grape --local-dir ./demo_data/so100_strawberry_grape\n",
    "```\n",
    "\n",
    "### Second, copy the modality file\n",
    "\n",
    "The `modality.json` file provides extra information about the state and action modalities to make the dataset ‚ÄúGR00T-compatible‚Äù. Copy `examples/so100__modality.json` into `<DATASET_PATH>/meta/modality.json`.\n",
    "\n",
    "```bash\n",
    "cp examples/so100__modality.json ./demo_data/so100_strawberry_grape/meta/modality.json\n",
    "```\n",
    "\n",
    "You can now load the dataset with the `LeRobotSingleDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31faf747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.utils.misc import any_describe\n",
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
    "\n",
    "dataset_path = \"./demo_data/so100_strawberry_grape\"   # change this to your dataset path\n",
    "\n",
    "data_config = DATA_CONFIG_MAP[\"so100\"]\n",
    "\n",
    "dataset = LeRobotSingleDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    modality_configs=data_config.modality_config(),\n",
    "    embodiment_tag=\"new_embodiment\",\n",
    "    video_backend=\"torchvision_av\",\n",
    ")\n",
    "\n",
    "resp = dataset[7]\n",
    "any_describe(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdde0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the dataset\n",
    "# show img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    if i % 10 == 0:\n",
    "        resp = dataset[i]\n",
    "        img = resp[\"video.webcam\"][0]\n",
    "        images_list.append(img)\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.imshow(images_list[i])\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Image {i}\")\n",
    "plt.tight_layout() # adjust the subplots to fit into the figure area.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f6681",
   "metadata": {},
   "source": [
    "## Step 2: Fine-Tuning\n",
    "\n",
    "Fine-tuning is done with our script `scripts/gr00t_finetune.py`.\n",
    "\n",
    "```bash\n",
    "python scripts/gr00t_finetune.py \\\n",
    "   --dataset-path /datasets/so100_strawberry_grape/ \\\n",
    "   --num-gpus 1 \\\n",
    "   --output-dir ~/so100-checkpoints \\\n",
    "   --max-steps 2000 \\\n",
    "   --data-config so100 \\\n",
    "   --video-backend torchvision_av\n",
    "```\n",
    "\n",
    "> **Tip**: Default settings require ~25 GB of VRAM.  \n",
    "> If you have less VRAM, add `--no-tune_diffusion_model` and/or lower `--batch-size` to avoid OOM errors .\n",
    "\n",
    "## Step 3: Open-loop Evaluation\n",
    "\n",
    "After training, visualize the fine-tuned policy:\n",
    "\n",
    "```bash\n",
    "python scripts/eval_policy.py --plot \\\n",
    "   --embodiment_tag new_embodiment \\\n",
    "   --model_path <YOUR_CHECKPOINT_PATH> \\\n",
    "   --data_config so100 \\\n",
    "   --dataset_path /datasets/so100_strawberry_grape/ \\\n",
    "   --video_backend torchvision_av \\\n",
    "   --modality_keys single_arm gripper\n",
    "```\n",
    "\n",
    "Here is a plot after 7 000 training steps:\n",
    "\n",
    "![so100-7k-steps.png](../media/so100-7k-steps.png)\n",
    "\n",
    "With more steps the curves improve noticeably.\n",
    "\n",
    "üéâ Great! You have successfully fine-tuned GR00T-N1 on a new embodiment.\n",
    "\n",
    "## Deployment\n",
    "\n",
    "For deployment details, see the notebook `5_policy_deployment.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c2a7b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Fine-Tuning Tutorial on G1 Block-Stacking Dataset\n",
    "\n",
    "This section provides a step-by-step guide to fine-tune GR00T-N1 on the G1 block-stacking dataset.\n",
    "\n",
    "## Step 1: Dataset\n",
    "\n",
    "Loading any dataset for fine-tuning is a two-step process:\n",
    "\n",
    "- **1.1** Define the modality configuration and transforms for the dataset  \n",
    "- **1.2** Load the dataset with the `LeRobotSingleDataset` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de821495",
   "metadata": {},
   "source": [
    "### Step 1.0: Download the Dataset\n",
    "\n",
    "- Download the dataset from:  \n",
    "  [https://huggingface.co/datasets/unitreerobotics/G1_BlockStacking_Dataset](https://huggingface.co/datasets/unitreerobotics/G1_BlockStacking_Dataset)\n",
    "\n",
    "- Copy `examples/unitree_g1_blocks__modality.json` into `<DATASET_PATH>/meta/modality.json`.  \n",
    "  This supplies extra metadata about state and action modalities so the dataset becomes ‚ÄúGR00T-compatible‚Äù.\n",
    "\n",
    "```bash\n",
    "cp examples/unitree_g1_blocks__modality.json datasets/G1_BlockStacking_Dataset/meta/modality.json\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding the Modality Configuration\n",
    "\n",
    "The file provides detailed metadata for state and action modalities, enabling:\n",
    "\n",
    "- **Decoupled storage and interpretation**  \n",
    "  - **States & actions**: Stored as concatenated float32 arrays. `modality.json` maps these arrays into distinct, semantically meaningful fields with training hints.  \n",
    "  - **Videos**: Stored as separate files; the config renames them to a canonical format.  \n",
    "  - **Annotations**: Tracks all annotation fields. Omit the `annotation` key if no annotations exist.\n",
    "\n",
    "- **Fine-grained slicing** ‚Äì splits arrays into semantically meaningful fields.  \n",
    "- **Clear mapping** ‚Äì explicit dimension mapping.  \n",
    "- **Complex transforms** ‚Äì per-field normalization and rotation transforms at training time.\n",
    "\n",
    "#### Schema\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"state\": {\n",
    "        \"<state_name>\": {\n",
    "            \"start\": <int>,   // start index in the state array\n",
    "            \"end\":   <int>    // end index in the state array\n",
    "        }\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"<action_name>\": {\n",
    "            \"start\": <int>,   // start index in the action array\n",
    "            \"end\":   <int>    // end index in the action array\n",
    "        }\n",
    "    },\n",
    "    \"video\": {\n",
    "        \"<video_name>\": {}   // empty dict for consistency\n",
    "    },\n",
    "    \"annotation\": {\n",
    "        \"<annotation_name>\": {}   // empty dict for consistency\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "An example can be found at `getting_started/examples/unitree_g1_blocks__modality.json`; place it inside the dataset‚Äôs `meta` folder.\n",
    "\n",
    "---\n",
    "\n",
    "### Generate Dataset Statistics\n",
    "\n",
    "Create `meta/metadata.json` by running:\n",
    "\n",
    "```bash\n",
    "python scripts/load_dataset.py \\\n",
    "  --data_path /datasets/G1_BlockStacking_Dataset/ \\\n",
    "  --embodiment_tag new_embodiment\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.schema import EmbodimentTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b34097",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./demo_data/g1\"  # change this to your dataset path\n",
    "embodiment_tag = EmbodimentTag.NEW_EMBODIMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c43ab9b",
   "metadata": {},
   "source": [
    "### Step 1.1: Modality Configuration & Transforms\n",
    "\n",
    "The modality configuration lets you cherry-pick exactly which data streams‚Äîvideo, state, action, language, etc.‚Äîare used during fine-tuning, giving you fine-grained control over which parts of the dataset the model sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4379813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.dataset import ModalityConfig\n",
    "\n",
    "\n",
    "# select the modality keys you want to use for finetuning\n",
    "video_modality = ModalityConfig(\n",
    "    delta_indices=[0],\n",
    "    modality_keys=[\"video.cam_right_high\"],\n",
    ")\n",
    "\n",
    "state_modality = ModalityConfig(\n",
    "    delta_indices=[0],\n",
    "    modality_keys=[\"state.left_arm\", \"state.right_arm\", \"state.left_hand\", \"state.right_hand\"],\n",
    ")\n",
    "\n",
    "action_modality = ModalityConfig(\n",
    "    delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    modality_keys=[\"action.left_arm\", \"action.right_arm\", \"action.left_hand\", \"action.right_hand\"],\n",
    ")\n",
    "\n",
    "language_modality = ModalityConfig(\n",
    "    delta_indices=[0],\n",
    "    modality_keys=[\"annotation.human.task_description\"],\n",
    ")\n",
    "\n",
    "modality_configs = {\n",
    "    \"video\": video_modality,\n",
    "    \"state\": state_modality,\n",
    "    \"action\": action_modality,\n",
    "    \"language\": language_modality,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc970e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.transform.base import ComposedModalityTransform\n",
    "from gr00t.data.transform import VideoToTensor, VideoCrop, VideoResize, VideoColorJitter, VideoToNumpy\n",
    "from gr00t.data.transform.state_action import StateActionToTensor, StateActionTransform\n",
    "from gr00t.data.transform.concat import ConcatTransform\n",
    "from gr00t.model.transforms import GR00TTransform\n",
    "\n",
    "\n",
    "# select the transforms you want to apply to the data\n",
    "to_apply_transforms = ComposedModalityTransform(\n",
    "    transforms=[\n",
    "        # video transforms\n",
    "        VideoToTensor(apply_to=video_modality.modality_keys, backend=\"torchvision\"),\n",
    "        VideoCrop(apply_to=video_modality.modality_keys, scale=0.95, backend=\"torchvision\"),\n",
    "        VideoResize(apply_to=video_modality.modality_keys, height=224, width=224, interpolation=\"linear\", backend=\"torchvision\" ),\n",
    "        VideoColorJitter(apply_to=video_modality.modality_keys, brightness=0.3, contrast=0.4, saturation=0.5, hue=0.08, backend=\"torchvision\"),\n",
    "        VideoToNumpy(apply_to=video_modality.modality_keys),\n",
    "\n",
    "        # state transforms\n",
    "        StateActionToTensor(apply_to=state_modality.modality_keys),\n",
    "        StateActionTransform(apply_to=state_modality.modality_keys, normalization_modes={\n",
    "            \"state.left_arm\": \"min_max\",\n",
    "            \"state.right_arm\": \"min_max\",\n",
    "            \"state.left_hand\": \"min_max\",\n",
    "            \"state.right_hand\": \"min_max\",\n",
    "        }),\n",
    "\n",
    "        # action transforms\n",
    "        StateActionToTensor(apply_to=action_modality.modality_keys),\n",
    "        StateActionTransform(apply_to=action_modality.modality_keys, normalization_modes={\n",
    "            \"action.right_arm\": \"min_max\",\n",
    "            \"action.left_arm\": \"min_max\",\n",
    "            \"action.right_hand\": \"min_max\",\n",
    "            \"action.left_hand\": \"min_max\",\n",
    "        }),\n",
    "\n",
    "        # ConcatTransform\n",
    "        ConcatTransform(\n",
    "            video_concat_order=video_modality.modality_keys,\n",
    "            state_concat_order=state_modality.modality_keys,\n",
    "            action_concat_order=action_modality.modality_keys,\n",
    "        ),\n",
    "        # model-specific transform\n",
    "        GR00TTransform(\n",
    "            state_horizon=len(state_modality.delta_indices),\n",
    "            action_horizon=len(action_modality.delta_indices),\n",
    "            max_state_dim=64,\n",
    "            max_action_dim=32,\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c328a7",
   "metadata": {},
   "source": [
    "### Step 1.2: Load the Dataset\n",
    "\n",
    "First, we‚Äôll visualize the dataset; then we‚Äôll load it with the `LeRobotSingleDataset` class (without transforms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "\n",
    "train_dataset = LeRobotSingleDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    modality_configs=modality_configs,\n",
    "    embodiment_tag=embodiment_tag,\n",
    "    video_backend=\"torchvision_av\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use matplotlib to visualize the images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(train_dataset[0].keys())\n",
    "\n",
    "images = []\n",
    "for i in range(5):\n",
    "    image = train_dataset[i][\"video.cam_right_high\"][0]\n",
    "    # image is in HWC format, convert it to CHW format\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    images.append(image)   \n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5))\n",
    "for i, image in enumerate(images):\n",
    "    axs[i].imshow(np.transpose(image, (1, 2, 0)))\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d15e69",
   "metadata": {},
   "source": [
    "Now we initialize the dataset with our modality configuration and transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LeRobotSingleDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    modality_configs=modality_configs,\n",
    "    embodiment_tag=embodiment_tag,\n",
    "    video_backend=\"torchvision_av\",\n",
    "    transforms=to_apply_transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e83d9",
   "metadata": {},
   "source": [
    "**Additional Notes**  \n",
    "- We use a **cached data loader** to accelerate training. It loads the entire dataset into memory, which greatly improves throughput. If your dataset is very large or you encounter out-of-memory (OOM) errors, simply switch to the standard LeRobot data loader (`gr00t.data.dataset.LeRobotSingleDataset`). Both loaders share the same API, so you can toggle between them without changing your code.  \n",
    "- The **video backend** is set to `torchvision_av`, which employs the `av` codec instead of the default h264."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc7af0",
   "metadata": {},
   "source": [
    "### Step 2: Load the Model\n",
    "\n",
    "Training proceeds in three stages:\n",
    "- **2.1** Load the base model from Hugging Face or a local path  \n",
    "- **2.2** Prepare training parameters  \n",
    "- **2.3** Run the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bfb89",
   "metadata": {},
   "source": [
    "#### Step 2.1: Load the Base Model\n",
    "\n",
    "We will load the model using the `from_pretrained_for_tuning` method, which lets us specify exactly which parts of the model to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca380f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5072f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.model.gr00t_n1 import GR00T_N1\n",
    "\n",
    "BASE_MODEL_PATH = \"nvidia/GR00T-N1-2B\"\n",
    "TUNE_LLM = False            # Whether to tune the LLM\n",
    "TUNE_VISUAL = True          # Whether to tune the visual encoder\n",
    "TUNE_PROJECTOR = True       # Whether to tune the projector\n",
    "TUNE_DIFFUSION_MODEL = True # Whether to tune the diffusion model\n",
    "\n",
    "model = GR00T_N1.from_pretrained(\n",
    "    pretrained_model_name_or_path=BASE_MODEL_PATH,\n",
    "    tune_llm=TUNE_LLM,  # backbone's LLM\n",
    "    tune_visual=TUNE_VISUAL,  # backbone's vision tower\n",
    "    tune_projector=TUNE_PROJECTOR,  # action head's projector\n",
    "    tune_diffusion_model=TUNE_DIFFUSION_MODEL,  # action head's DiT\n",
    ")\n",
    "\n",
    "# Set the model's compute_dtype to bfloat16\n",
    "model.compute_dtype = \"bfloat16\"\n",
    "model.config.compute_dtype = \"bfloat16\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa0b0da",
   "metadata": {},
   "source": [
    "#### Step 2.2: Prepare Training Parameters\n",
    "\n",
    "We configure training with Hugging Face‚Äôs `TrainingArguments`. Key parameters are outlined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c69e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"output/model/path\"    # CHANGE THIS ACCORDING TO YOUR LOCAL PATH\n",
    "per_device_train_batch_size = 8     # CHANGE THIS ACCORDING TO YOUR GPU MEMORY\n",
    "max_steps = 20                      # CHANGE THIS ACCORDING TO YOUR NEEDS\n",
    "report_to = \"wandb\"\n",
    "dataloader_num_workers = 8\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    run_name=None,\n",
    "    remove_unused_columns=False,\n",
    "    deepspeed=\"\",\n",
    "    gradient_checkpointing=False,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    dataloader_num_workers=dataloader_num_workers,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_persistent_workers=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta1=0.95,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10.0,\n",
    "    num_train_epochs=300,\n",
    "    max_steps=max_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_total_limit=8,\n",
    "    report_to=report_to,\n",
    "    seed=42,\n",
    "    do_eval=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    ddp_bucket_cap_mb=100,\n",
    "    torch_compile_mode=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9482b2",
   "metadata": {},
   "source": [
    "#### Step 2.3: Initialize the Training Runner and Launch the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.experiment.runner import TrainRunner\n",
    "\n",
    "experiment = TrainRunner(\n",
    "    train_dataset=train_dataset,\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    ")\n",
    "\n",
    "experiment.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e487c",
   "metadata": {},
   "source": [
    "We can compare the offline validation results after 1 000 steps versus 10 000 steps:\n",
    "\n",
    "**Fine-tuning Results on the Unitree G1 Block-Stacking Dataset**\n",
    "\n",
    "| 1 k steps | 10 k steps |\n",
    "|-----------|------------|\n",
    "| ![1k](../media/g1_ft_1k.png) | ![10k](../media/g1_ft_10k.png) |\n",
    "| MSE: 0.0181 | MSE: 0.0022 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
