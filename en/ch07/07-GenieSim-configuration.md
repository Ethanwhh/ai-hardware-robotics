Genie Sim is a simulation framework launched by AgiBot that provides developers with efficient data generation capabilities and evaluation benchmarks to accelerate the development of embodied intelligence. Genie Sim establishes a comprehensive closed-loop process, including trajectory generation, model training, benchmark testing, and deployment validation. Users can quickly verify algorithm performance and optimize models through this efficient simulation toolchain. Whether for simple grasping tasks or complex remote operations, Genie Sim provides highly realistic simulation environments and precise evaluation metrics to help developers efficiently complete the development and iteration of robotic technologies.
Genie Sim Benchmark, as the open-source evaluation version of Genie Sim, is dedicated to providing accurate performance testing and optimization support for embodied AI models.
You can start by downloading the assets

```Bash
sudo apt install git-lfsgit lfs install# When prompted for a password, use an access token with write permissions.# Generate one from your settings: https://huggingface.co/settings/tokensgit clone https://huggingface.co/datasets/agibot-world/GenieSimAssets
```

```Bash
# Configure the repositorycurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \    && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \    && \    sudo apt-get update# Install the NVIDIA Container Toolkit packagessudo apt-get install -y nvidia-container-toolkitsudo systemctl restart docker# Configure the container runtimesudo nvidia-ctk runtime configure --runtime=dockersudo systemctl restart docker# Verify NVIDIA Container Toolkitdocker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi
```

> [https://docs.isaacsim.omniverse.nvidia.com/latest/installation/requirements.html](https://docs.isaacsim.omniverse.nvidia.com/latest/installation/requirements.html)

### [2.2 Download Scenes and Assets from Huggingface](https://agibot-world.com/sim-evaluation/docs/#/?id=_22-download-scenes-amp-assets-from-huggingface)

Please visit [https://huggingface.co/datasets/agibot-world/GenieSimAssets](https://huggingface.co/datasets/agibot-world/GenieSimAssets) and follow the instructions.

### [2.3 Installation](https://agibot-world.com/sim-evaluation/docs/#/?id=_23-installation)

#### [2.3.1 Docker Container (Recommended)](https://agibot-world.com/sim-evaluation/docs/#/?id=_231-docker-container-recommended)

1. Develop using Docker container
   - Install Docker according to [Isaac Sim documentation](https://docs.isaacsim.omniverse.nvidia.com/latest/installation/install_container.html)
2. Prepare Docker image
   
   ```bash
   
   ```

```Plain
# Navigate to genie_sim root directory and create Docker image from Dockerfiledocker build -f ./scripts/dockerfile -t registry.agibot.com/genie-sim/open_source:latest .
```

If you encounter network issues, my solution is:

```bash
ip addr show docker0 | grep "inet\s" | awk '{print $2}' | cut -d/ -f1DOCKER_HOST_IP="172.17.0.1" docker build \
  --add-host=host.docker.internal:"$DOCKER_HOST_IP" \
  --build-arg http_proxy="http://host.docker.internal:7890" \
  --build-arg https_proxy="http://host.docker.internal:7890" \
  --build-arg no_proxy="localhost,127.0.0.1" \
  -f ./scripts/dockerfile \
  -t registry.agibot.com/genie-sim/open_source:latest .
```

- Run Docker container and start server

```Bash
# Start a new container in main directory# You need to change ~/assets to GenieSimAssets folderSIM_ASSETS=~/17robo/GenieSimAssets/ ./scripts/start_gui.sh./scripts/into.sh# Start serveromni_python server/source/genie.sim.lab/raise_standalone_sim.py --enable_curobo True  # Grasping trajectory for this demo is generated by Curobo
```

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=MzY4NDA5OGE2NjYyYjE5NzVmMzg0OTE3Y2YxNjlhNDNfWGdRcUQ4dXdIV3k0aVh1UFY2UnM5dmpZYTRnRWJYQnRfVG9rZW46WEFxS2JnbWdtb3A5Uk14VlRQZ2Mycm9Vbk5lXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

Execute the next command after App is ready

Then we can see that if the supermarket demo completes, the simulator here will also exit.

```Bash
# start container in main directory./scripts/into.sh# start client in containeromni_python benchmark/task_benchmark.py --task_name=curobo_restock_supermarket_items --env_class=DemoEnvomni_python benchmark/task_benchmark.py --task_name=iros_open_drawer_and_store_items --env_class=DemoEnv  (currently has errors)omni_python benchmark/task_benchmark.py --task_name=genie_task_home_microwave_food --env_class=DemoEnv (this also doesn't work, only outputs steps)geniesim2.1 and 2.0 both provide only one example of running tasks with curobo, and the open-source part only configures the restocking task. Other iros tasks require model inference or teleoperation
```

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=MzAwYTRmNWY2OGZmZWExZGQ1ZmIyNTczMzMyNzM5ZTlfUWxFalpScE8zNzFWVUJGN2JQb1JINkJ3YjhEeXhhNkNfVG9rZW46U0tJRWJtMU93b1BCWER4SDhQbGNpckp1bkdoXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

This green info will appear first

Then wait for about 1 minute

If you encounter gRPC-related errors, please refer to the issue for modifications

https://github.com/AgibotTech/genie_sim/issues/11

Note: Do not set any proxies in the command line! Also run sudo ufw allow 50051

```Bash
make sure your server is running (launched by omni_python server/source/genie.sim.lab/raise_standalone_sim.py --enable_curobo True in same container)make sure your port 50051 is not be used. You must kill other process on 50051 by sudo lsof -i :50051 and sudo kill -9 .
```

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=NTIxZGI3ZjllNThiMzUwODg4NmFkMzIxZTE2ZWMyNjRfWWZWaGZEd0tIWm9PM1NXSGtPMFJqZklFMVdIT3hDVnRfVG9rZW46UndCWGJkWUdFb3JBeFB4RkdFYWNXa1BRbmRmXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=NjM0YTRiNjlmYjc0MTcyOWNjZTQ4ZjM5NTVhMGU4YjVfaUFwRWFtOElyVHcwaDRCY2FvRmlRYXFrWFp1WGVicXpfVG9rZW46TUpCcmJMYzA4b2FqRFV4M1dqcWNTUVZNbklkXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

I encountered a problem where it got stuck here

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=MTg1YmE4M2YzYzFmNDVkM2Y5NDczNjE1N2YxYTg4ZDJfSTdrYUcxdjNTSFRkQ3ZPSnAybks4VXdZWjRLMHFjRzFfVG9rZW46WFNsd2JFaDBEb1plNWt4d3BHWGNFempabjdjXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

Pull the latest code from remote.

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=NmM3ODdlZDIxZWMyMWEwZjdiZDQ0Y2YyOTA1YzNmNTRfTXFaalo0cWVGT051REtXOXdwRGxhelZjcEtNVUExQVRfVG9rZW46RUdZeGJ2YVpSbzY1Um14bXJXeGNzbVp1bkdlXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

Then it won't get stuck.

Also, if there are corresponding large file changes, you can go to assets:

git lfs pull *# Make sure all LFS files are up to date*

https://github.com/AgibotTech/genie_sim/issues/29

#### [2.3.2 Host Machine](https://agibot-world.com/sim-evaluation/docs/#/?id=_232-host-machine)

1. We recommend developers use our unified Docker container environment.

2. If you wish to use your own environment, please refer to our provided `dockerfile` and install the dependencies we have listed.

#### [2.3.3 Developer Guide](https://agibot-world.com/sim-evaluation/docs/#/?id=_233-developer-guide)

#### [2.3.4 Enable pre-commit Hooks (Optional)](https://agibot-world.com/sim-evaluation/docs/#/?id=_234-enable-pre-commit-hooks-optional)

1. Install and set up `pre-commit` to enable automatic file formatters for Python / JSON / YAML etc.

```Bash
# Install pre-commit into your Python environmentsudo apt install python3-pippip3 install pre-commit# Enable predefined pre-commit hooks in the repositorypre-commit install
```

2. Trigger file formatters for all tracked files

```Bash
pre-commit run --all-files
```

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=MGRhOTM4N2Y3YTI2YWFmN2I2ZjE4OGIyNzAzOWEwNzRfS1FLalM5dllzS3hiV1RnOVFyUm5EOTNkMFdSbDY4UjNfVG9rZW46RUhpNWJ0NlRHb3VFdXF4bWh2V2NNUlhwbkVnXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

### [2.4 Benchmark Tasks](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_24-benchmark-tasks)#### [2.4.1 Run Benchmark](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_241-run-benchmark)1. Run docker container

```Plain
# Start a new container in main directory# You need to change ~/assets to GenieSimAssets folderSIM_ASSETS=~/17robot/GenieSimAssets ./scripts/start_gui.sh
```

2. Run benchmark

Execute the following command in the `genie_sim` root directory outside the docker container

Auto run is quite useful. However, it requires the infer parameter

```Plain
./scripts/autorun.sh genie_task_home_pour_water infer
```

https://github.com/AgibotTech/genie_sim/issues/34

Supports the following task names

**Task Names**

genie_task_cafe_espresso
genie_task_cafe_toast
genie_task_home_clean_desktop
genie_task_home_collect_toy
genie_task_home_microwave_food
genie_task_home_open_drawer
genie_task_home_pass_water
genie_task_home_pour_water
genie_task_home_wipe_dirt
genie_task_supermarket_cashier_packing
genie_task_supermarket_stock_shelf
genie_task_supermarket_pack_fruit

---

#### [2.4.2 Benchmark Evaluation Framework](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_242-benchmark-evaluation-framework)

Configuration

Use ADER (Action Domain Evaluation Rule) for evaluation configuration

**Example**

JSON

```Plain
{
  "Acts": [
    {
      "ActionList": [
        {
          "ActionSetWaitAny": [
            {
              "Follow": "beverage_bottle_002|[0.2,0.2,0.2]|right"
            },
            {
              "Timeout": 120
            },
            {
              "Onfloor": "beverage_bottle_002|0.0"
            }
          ]
        },
        {
          "ActionSetWaitAny": [
            {
              "PickUpOnGripper": "beverage_bottle_002|right"
            },
            {
              "Timeout": 120
            },
            {
              "Onfloor": "beverage_bottle_002|0.0"
            }
          ]
        },
        {
          "ActionSetWaitAny": [
            {
              "Follow": "handbag_000|[0.4,0.4,0.4]|right"
            },
            {
              "Timeout": 120
            }
          ]
        },
        {
          "ActionSetWaitAny": [
            {
              "Inside": "beverage_bottle_002|handbag_000|1"
            },
            {
              "StepOut": 1000
            }
          ]
        }
      ]
    }
  ],
  "Init": [],
  "Objects": [
    {
      "bottle": [
        "beverage_bottle_002"
      ],
      "handbag": [
        "handbag_000"
      ]
    }
  ],
  "Problem": "pack_in_the_supermarket"
}
```

**Current Evaluation Capabilities**

<style> td {white-space:nowrap;border:0.5pt solid #dee0e3;font-size:10pt;font-style:normal;font-weight:normal;vertical-align:middle;word-break:normal;word-wrap:normal;}</style>

|                   |                                                   |                  |                                 |
| ----------------- | ------------------------------------------------- | ---------------- | ------------------------------- |
| Action            | Description                                       | Base Class       | Syntax                          |
| Common (COMMON)   |                                                   |                  |                                 |
| ActionList        | Queue action: internal actions execute sequentially | ActionBase       | "ActionList":[]                 |
| ActionSetWaitAny  | Conditional queue action: completes when any internal action completes | ActionBase       | "ActionSetWaitAny":[]           |
| ActionWaitForTime | Time wait action: similar to sleep but doesn't block threads | ActionBase       | "ActionWaitForTime": 3.0        |
| TimeOut           | Timeout verification action: checks if timeout occurs | ActionCancelBase | "Timeout": 60                   |
| StepOut           | Step limit verification action: checks if step limit is reached | ActionCancelBase | "StepOut": 100                  |
| ActionSetWaitAll  | Exits when all conditions are met                 | ActionBase       | "ActionSetWaitAll":[]           |
| Custom (CUSTOM)   |                                                   |                  |                                 |
| Ontop             | One object is on top of another                   | EvaluateAction   | "Ontop": "active_obj            |
| Inside            | One object is inside another                      | EvaluateAction   | "Inside": "active_obj           |
| PushPull          | Checks if sliding joint of articulated object is within threshold [min, max] - used to determine if drawer-like object is open or closed | EvaluateAction   | "PushPull": "obj_id             |
| Follow            | Checks if left/right gripper is following specific object within bounding box range [x, y, z] | EvaluateAction   | "Follow": "obj_id               |
| PickUpOnGripper   | Gripper picks up an object                        | EvaluateAction   | "PickUpOnRightGripper": "object |
| OnShelf           | Object is within specific area                    | EvaluateAction   |                                 |
| Onfloor           | Checks if specified object has fallen below reference height ref_z; if yes, exits | ActionCancelBase | "Inside": "obj_id              |
| Cover             | Object A covers Object B                          | EvaluateAction   | "Cover": "active_obj            |

**Evaluation Output Data Structure**

JSON

```Plain
// Output
[
  {
    "task_type": "benchmark",
    "model_path": "",
    "task_uid": "13513cf3-2d88-421e-b3e5-dc0998a60970",
    "task_name": "genie_task_supermarket",
    "stage": "",
    "result": {
      "code": -1,
      "step": 0,
      "msg": "",
      "progress": [],
      "scores":[]
    },
    "start_time": "2025-06-05 15:04:31",
    "end_time": "2025-06-05 15:05:40"
  }
]
```

**Error Codes**

Python

```Plain
from enum import Enum
class ErrorCode(Enum):
    INIT_VALUE = -1
    SUCCESS = 0
    ABNORMAL_INTERRUPTION = 1
    OUT_OF_MAX_STEP = 2
    UNKNOWN_ERROR = 500
```

---

### [2.5 AgiBot World Challenge Manipulation Tasks](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_25-agibot-world-challenge-manipulation-tasks)

#### [2.5.1 Run Baseline Model Inference](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_251-run-baseline-model-inference)

1. Download AgiBot World repository and baseline model from the following address

2. a. [GitHub - OpenDriveLab/AgiBot-World at manipulation-challenge](https://github.com/OpenDriveLab/AgiBot-World/tree/manipulation-challenge)

After downloading, as long as docker doesn't update the build, the previous agibot-world will still be under docker's main directory

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=Y2M2YTkzMDM3ZmYzNTJiOTYyNTc1YjQ4Njc1MDIyMzBfN1RIc1ppWENYR0M2SFpoMnRRRzZtTGJTS0xZcWg1WFJfVG9rZW46SmRDaGJWckJTb1JudGh4ZnoxcmNRb1FxbnZkXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

```Plain
# Initialize Agibot World repository to run baseline model
git clone -b git submodule update --init --recursive
```

You can pull updates each time afterwards:

```TypeScript
root@ubuntu22:~/workspace/main/AgiBot-World# lsexperiments       InternVL             __pycache__       scriptsgenie_sim_ros.py  latent_action_model  README.mdhubconf.py        prismatic            requirements.txtroot@ubuntu22:~/workspace/main/AgiBot-World# git pullfatal: detected dubious ownership in repository at '/root/workspace/main/AgiBot-World'To add an exception for this directory, call:
        git config --global --add safe.directory /root/workspace/main/AgiBot-Worldroot@ubuntu22:~/workspace/main/AgiBot-World# Need to rungit config --global --add safe.directory /root/workspace/main/AgiBot-WorldThen git pullgit submodule update --init --recursive
```

FileNotFoundError: [Errno 2] No such file or directory: 'checkpoints/finetuned/action_decoder.pt'

There was an error running python

This is because the baseline model needs to be added

3. Move all baseline model files and code to Agibot-World directory

```Bash
cp -r UniVLA/latent_action_model/ ./latent_action_model/cp -r UniVLA/scripts/ ./scripts/cp -r UniVLA/prismatic/ ./prismatic/cp -r UniVLA/experiments/ ./experiments/mkdir -p checkpoints/finetunedcd checkpoints/finetunedgit lfs clone https://huggingface.co/qwbu/univla-iros-manipulation-challenge-baselinemv univla-iros-manipulation-challenge-baseline/* ./
```

To keep up to date

```Bash
git clone -b manipulation-challenge https://github.com/OpenDriveLab/AgiBot-World.git AgiBot-World-manipulation-challengecp -r AgiBot-World-manipulation-challenge/UniVLA/* AgiBot-World/
```

```Plain
main├── AgiBot-World│   ├── InternVL│   ├── experiments/robot│   ├── latent_action_model│   ├── prismatic│   ├── robot│   ├── scripts│   │   └── infer.py│   │   └── ...│   │   └── ...│   ├── checkpoints│   │   └── finetuned│   │       └── readme.txt│   ├── genie_sim_ros.py│   ├── hubconf.py│   └── requirements.txt
```

There's a place that needs modification - line 5 of infer.py:

I'll submit a PR when I have time, and welcome other contributors to submit as well (this issue has been fixed, so no need to worry about it now)

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=ZGZiZmRhOWY4MzRkNGI2MDk1OTcxMzQ0YjFlMjI3NGJfc01FTEhJZ1NWVzd6Ym4xWUNOTGI5ZU5RdzdqWDhBcGRfVG9rZW46Sm5USWJUZjd0b0JEUTZ4RnJ2eGNGS21WbnhmXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

4. Run docker container

```Plain
# You need to change ~/assets to GenieSimAssets folderSIM_ASSETS=~/17robo/GenieSimAssets ./scripts/start_gui.sh
```

4. Run baseline model inference

Execute the following command in the `genie_sim` root directory outside the docker container

```Plain
./scripts/autorun.sh iros_open_drawer_and_store_items infer./scripts/autorun.sh iros_clear_the_countertop_waste infer
```

Supports the following task names

<style> td {white-space:nowrap;border:0.5pt solid #dee0e3;font-size:10pt;font-style:normal;font-weight:normal;vertical-align:middle;word-break:normal;word-wrap:normal;}</style>

|                                        |
| -------------------------------------- |
| Task Name                              |
| iros_clear_the_countertop_waste        |
| iros_open_drawer_and_store_items       |
| iros_heat_the_food_in_the_microwave    |
| iros_pack_moving_objects_from_conveyor |
| iros_pickup_items_from_the_freezer     |
| iros_restock_supermarket_items         |
| iros_pack_in_the_supermarket           |
| iros_make_a_sandwich                   |
| iros_clear_table_in_the_restaurant     |
| iros_stamp_the_seal                    |

---

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=ODQwNTAzY2FkMzJiMGEzYjNiYzljMjE1MWFhZWE0NjZfc20yaTVlcXA3RTU5cFp4c2Z5M0UzcEtVQzRwUkxhVzlfVG9rZW46QmRjT2IzcGpib05wUlh4OHRURmN6Tm1jbmJIXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

Success! A 24G graphics card is sufficient

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=MWY4NDZjNzUxNTUwZWM2YWZjOTRhNDY5ZTQ5ODI0MDdfNnJLVmlnOWZsZmdOdUVvVmI2c1ZHb2NUZTE5RWpXTllfVG9rZW46VWNmeWJ5RW9Lb3Q0VFR4RDQxS2M4M3h0blVmXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=OTJmMmVkODg4ZDdmZTg3ODBiODVmM2JiYzQxZDNkZjhfcVZCSUFpTU55ajBJT2h5VGc1Y2NlZ0RXME1wQ2ZvakhfVG9rZW46WXdMWWJZOVBSb1dEaXh4eTVlTGNxN21XbkplXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

Opening the drawer and grasping objects, though there are still many issues during the process, and finally the Rubik's cube ended up on the floor

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=NzQwZmM3ZGIxOTMxMDE3MmNlZmIwODY1NGMxZDY2YjhfeGlGbVFXMXYwRzQ3RXpSclJON08yb0RsT3dkYzk3M0dfVG9rZW46VG5IU2JSWUNYb0sxZjB4eWhVeWNXSU14blBkXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

#### [2.5.2 Integrate Your Own Policy](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_252-integrate-your-own-policy)

1. Build your own code according to the following instructions

2. a. Start your model integration with the template at the following path. Do not modify the structure of the `infer` function.

```Plain
main├── model│   ├──demo_infer.py
```

b. When customizing, ensure your ROS node strictly follows the specified topics

*Communication between model inference and simulation environment is standardized and implemented based on ROS2. These are dedicated topics specified for tasks.*

<style> td {white-space:nowrap;border:0.5pt solid #dee0e3;font-size:10pt;font-style:normal;font-weight:normal;vertical-align:middle;word-break:normal;word-wrap:normal;}</style>

|                            |      |      |                 |                   |
| -------------------------- | ---- | ---- | --------------- | ----------------- |
| Topic Name                 | Publisher | Subscriber | Message Type            | Content                |
| /joint_command             | Model   | Simulation Environment | JointState      | Joint commands output by model inference       |
| /joint_states              | Simulation Environment | Model   | JointState      | Current robot joint states from simulation    |
| /sim/head_img              | Simulation Environment | Model   | CompressedImage | Current head RGB image from simulation  |
| /sim/left_wrist_img        | Simulation Environment | Model   | CompressedImage | Current left wrist RGB image from simulation |
| /sim/right_wrist_img       | Simulation Environment | Model   | CompressedImage | Current right wrist RGB image from simulation |
| /sim/head_depth_img        | Simulation Environment | Model   | CompressedImage | Current head depth image from simulation     |
| /sim/left_wrist_depth_img  | Simulation Environment | Model   | CompressedImage | Current left wrist depth image from simulation    |
| /sim/right_wrist_depth_img | Simulation Environment | Model   | CompressedImage | Current right wrist depth image from simulation   |

c. After customization, confirm that the file organization matches the specified layout and ensure `infer.py` and `genie_sim_ros.py` are included.

```Plain
main├── AgiBot-World│   ├── scripts│   │   └── infer.py│   │   └── ...│   │   └── ...│   ├──genie_sim_ros.py│   ├──...│   └...
```

2. Run docker container

Bash

```Plain
# You need to change ~/assets to GenieSimAssets folderSIM_ASSETS=~/assets ./scripts/start_gui.sh
```

3. Run model inference

4. Execute the following command in the genie_sim root directory outside the docker container

```Plain
./scripts/autorun.sh {TASK_NAME} infer
```

4. Model inference dependencies

Common Python libraries are listed in the `requirements.txt` file of the Genie Sim Benchmark repository and are pre-installed in the docker image of our operation test server. If you need additional Python libraries to run your policy, you can upload a `requirements.txt` file containing additional libraries along with your model files on the test server.

---

#### [2.5.3 Agibot World Challenge Evaluation Framework](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_253-agibot-world-challenge-evaluation-framework)

Please refer to 2.4.2 Benchmark Evaluation Framework

---

### [2.6 Teleoperation](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_26-teleoperation)

#### [2.6.1 PICO](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_261-pico)

Supports using a controller to control the robot's waist, head, left/right end effectors, and base movement.

**User Guide**

<style> td {white-space:nowrap;border:0.5pt solid #dee0e3;font-size:10pt;font-style:normal;font-weight:normal;vertical-align:middle;word-break:normal;word-wrap:normal;}</style>

|     |                           |
| --- | ------------------------- |
| Number | Function (Left / Right)        |
| ①   | Joystick: Move robot base<br>Press: Reset base posture |
| ②   | Reset left arm                      |
| ③   | Enable posture tracking                    |
| ④   | Reset right arm                      |
| ⑤   | Reset body and head                   |
| ⑥   | Control left gripper                     |

**Pico Setup**

1. Connect to the same **LAN** as the computer

2. Launch **AIDEA Vision App** in the library

3. Select **Wireless Connection** and enter the computer's **IP** address

**Launch Setup**

1. Start server

Bash

```Plain
# Run this command inside the containeromni_python server/source/genie.sim.lab/raise_standalone_sim.py
```

2. Start PICO control in container

Bash

```Plain
# Run this command inside the containeromni_python teleop/teleop.py --task_name genie_task_home_microwave_food --mode pico --host_ip x.x.x.x
```

---

#### [2.6.2 Keyboard](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_262-keyboard)

Supports keyboard control of robot waist, head, left/right end effectors, and base movement.

User Guide

<style> td {white-space:nowrap;border:0.5pt solid #dee0e3;font-size:10pt;font-style:normal;font-weight:normal;vertical-align:middle;word-break:normal;word-wrap:normal;}</style>

|        |         |          |        |
| ------ | ------- | -------- | ------ |
| Key    | Function      | Key       | Function     |
| w      | End effector forward | i        | Roll +   |
| s      | End effector backward | k        | Roll -   |
| a      | End effector left | j        | Pitch +   |
| d      | End effector right | l        | Pitch -   |
| q      | End effector up | u        | Yaw +   |
| e      | End effector down | o        | Yaw -   |
| ↑      | Base forward    | shift+↑  | Head pitch + |
| ↓      | Base backward    | shift+↓  | Head pitch - |
| ←      | Base left turn    | shift+←  | Head yaw + |
| →      | Base right turn    | shift+→  | Head yaw - |
| ctrl+↑ | Waist up    | ctrl+tab | Switch arm   |
| ctrl+↓ | Waist down    | r        | Reset     |
| ctrl+← | Waist pitch -  | c        | Close gripper   |
| ctrl+→ | Waist pitch +  | ctrl+c   | Open gripper   |

**Launch Setup**

1. Start server

Bash

```Plain
SIM_ASSETS=~/17robo/GenieSimAssets/ ./scripts/start_gui.sh./scripts/into.sh# Run this command inside the containeromni_python server/source/genie.sim.lab/raise_standalone_sim.py
```

2. Start keyboard control in container

Bash

```Plain
# Run this command inside the containeromni_python teleop/teleop.py --task_name genie_task_home_microwave_food --mode "keyboard"
```

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=YWJhOTlmYmY4NjRmMDgyOGJjNzZhNzc4N2JlMTQ0M2RfYzBLMlZWVWpuUHhNS3BoUGlsODMyQkw1cTNYUk4ySWZfVG9rZW46S3dnZmJqb3BUb3czMXF4MVFDZWNKekJEbjNiXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

Be careful with the waist control, it still needs improvement, but overall the end effector control works great

---

### [2.7 Recording & Playback](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_27-record-amp-replay)

Let me note the tasks again:

genie_task_cafe_espresso
genie_task_cafe_toast
genie_task_home_clean_desktop
genie_task_home_collect_toy
genie_task_home_microwave_food
genie_task_home_open_drawer
genie_task_home_pass_water
genie_task_home_pour_water
genie_task_home_wipe_dirt
genie_task_supermarket_cashier_packing
genie_task_supermarket_stock_shelf
genie_task_supermarket_pack_fruit

To improve efficiency, first record the trajectory, then perform playback to record the video.

Record all scene information (including robot joint positions, object poses, camera poses, etc.) in the state.json file.

- Start server

Bash

```Plain
# Run this command inside the containeromni_python server/source/genie.sim.lab/raise_standalone_sim.py
```

- Start client

Bash

```Plain
# Run this command inside the containeromni_python teleop/teleop.py --task_name genie_task_home_pour_water --mode keyboard  --record
```

Scene information is recorded in `./output/recording_data/{TASK_NAME}/state.json`

The generated /root/workspace/main/output/recording_data/genie_task_home_pour_water/state.json

#### [2.7.1 Replay](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_271-replay)

Replay trajectory and record video.

- Start server

Bash

```Plain
# Run this command inside the containeromni_python server/source/genie.sim.lab/raise_standalone_sim.py --disable_physics --record_img --record_video
```

- Start client

Bash

```Plain
# Example: TASK_NAME=genie_task_home_pour_water# Run this command inside the containeromni_python teleop/replay_state.py --task_file teleop/tasks/${TASK_NAME}.json --state_file output/recording_data/${TASK_NAME}/state.json --record
```

Images and videos are recorded in `./output/recording_data/{TASK_NAME}/{IDX}/`

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=NzczY2ExNGFhNzk1MDQwNTNjZjgzMDljYzdlMjc2ZTZfeGg0aDhLUmdqUlduM3ZkNDdKOUZsMVZzNjdiOUR4WTVfVG9rZW46QldEMWJtalpRb1VuWEh4UzZyMGNHb3RGbmlmXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

![](https://icndr2yneehy.feishu.cn/space/api/box/stream/download/asynccode/?code=MDM2YmY4OTI0MTkxNWNkY2MxNDFjY2FlZmM2MDVmNGZfbGJtNXJ0N0EzUHRLZjNQWndPNEpUd01sUG1ua0NqdFFfVG9rZW46S2ExaWJnYTE3bzhrY0t4TDdWcmNlalJpbjliXzE3NTE4NzYzOTQ6MTc1MTg3OTk5NF9WNA)

---

## [3. Use Cases](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_3-use-cases)

### [3.1 How to Run Simulation with Just One Line of Code](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_31-how-to-run-simulation-with-just-one-line-of-code)

1. Run docker container

Bash

```Plain
SIM_ASSETS=~/17robo/GenieSimAssets ./scripts/start_gui.sh
```

2. Execute shell script in `genie_sim` root directory outside docker container

3. a. Run task layout

Bash

```Plain
./scripts/autorun.sh {TASK_NAME}
```

b. Run PICO teleoperation

Bash

```Plain
./scripts/autorun.sh {TASK_NAME} pico {HOST_IP}
```

c. Run keyboard teleoperation

Bash

```Plain
./scripts/autorun.sh {TASK_NAME} keyboard
```

d. Run scene replay

Bash

```Plain
./scripts/autorun.sh {TASK_NAME} replay {STATE_FILE_PATH}
```

e. Run model inference

Bash

```Plain
./scripts/autorun.sh {TASK_NAME} infer
```

f. Run cleanup

Bash

```Plain
./scripts/autorun.sh clean
```

3. Press `q` or `Q` in the terminal to stop the task normally. The `autorun` script enables ros topic recording by default. If not needed, please remove the relevant code in the `autorun` script and ensure regular cleanup of the output folder. (What information is stored? Where is it saved?)

---

### [3.2 How to Set Up a Benchmark Task File](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_32-how-to-set-up-a-benchmark-task-file)

Each benchmark task file contains the following 6 key elements:

1. **task** contains a unique task name

2. **objects** contains multiple types of task objects:
   
   1. `extra_objects`: Non-interactive objects
   
   2. `fix_objects`: Interactive objects with fixed initial poses
   
   3. `task_related_objects`: Interactive objects with random initial poses

3. **recording_setting** specifies camera perspectives to record

4. **robot** includes robot ID, robot configuration file, and robot base initial pose

5. **scene** specifies a set of scene information:
   
   1. `scene_id`: Unique name of the scene
   
   2. `function_space_objects`: Cuboid area for randomly generating `task_related_objects`
   
   3. `scene_info_dir`: Path to scene assets
   
   4. `scene_usd`: Path to scene usd file

6. **stages** contains several sub-stages designed for planning tasks
   
   1. `action`: Defines action name
   
   2. `active`: Defines active objects, such as grippers
   
   3. `passive`: Defines passive objects, such as bottles

---

### [3.3 How to Create a Complete Benchmark Task with Embodied AI Model](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_33-how-to-create-a-complete-benchmark-task-with-embodied-ai-model)

1. First, assemble your task template, such as "genie_task_supermarket.json". This configuration file includes robot, scene, objects, stages, etc.

```Plain
benchmark/bddl/eval_tasks/your_task.json
```

Please correctly fill in the asset file paths. The program will look for assets in the user-configured environment variable `SIM_ASSETS`.

2. Scene generalization: Map your task with scenes

```Plain
File path: benchmark/bddl/task_to_preselected_scenes.json
```

JSON

```Plain
{
  "genie_task_supermarket": [
    "scenes/genie/supermarket_02/Collected_emptyScene_01/emptyScene_01.usd"
  ]
}
```

3. Write evaluation criteria code, for example

```Plain
File path: benchmark/bddl/task_definitions/genie_task_supermarket/problem0.bddl
```

Lisp

```Plain
(define (problem restock_shelves)
    (:domain isaac)
    (:objects
        benchmark_beverage_bottle_013 - bottle.n.01
    )
    (:init
        (onfloor benchmark_beverage_bottle_013 floor.n.01_2)
    )
    (:goal
        (onshelf ?benchmark_beverage_bottle_013)
    )
)
```

Now, the benchmark configuration steps are complete.

4. Access your embodied intelligence model in "yourpolicy.py". (Please refer to demopolicy.py)

Python
```python
class YourPolicy(BasePolicy):
    def __init__(self) -> None:
        super().__init__()
        """Initialize configuration and load the model."""
        pass

    def reset(self):
        """Reset."""
        pass

    def act(self, observations, **kwargs) -> np.ndarray:
        """Take action based on observations.

        Args:
            observations: Contains robot images (Head_Camera_01/Right_Camera_01/Left_Camera_01)
                          and current joint information.
        Returns:
            Target joint positions for the robot.
        """
        pass
```

**5. Finally, run the client and evaluate your own AI model**

```bash
python3 benchmark/task_benchmark.py --task_name {task name} --policy_class {policy name} --env_class OmniEnv
```

---

### [3.4 Best Practice of Asset Parameters for Manipulation Tasks](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_34-best-practice-of-asset-parameters-for-manipulation-task)

#### [3.4.1 How to Debug Colliders in IsaacSim](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_341-how-to-debug-collider-in-isaacsim)

> Follow the instructions to enable collider debug mode.

---

#### [3.4.2 Bad Examples](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_342-bad-examples)

- Complex meshes can lead to collision computation errors — simpler is better.  
- Too many triangle meshes in a small area often result in mesh collisions.  

**convexHull** and **convexDecomposition**

---

#### [3.4.3 Good Examples](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_343-good-examples)

"Good" means easy to grasp and minimal mesh collision.

- Good collision meshes are typically simple and clean.  
- Ensure the key grasping area is just right (e.g., tin can, bottle, bottle cap).

**convexDecomposition mesh**  
**convexDecomposition collider**  
**convexHull**

---

## [4. API Reference](https://agibot-world.com/sim-evaluation/docs/#/v2?id=_4-api-reference)

<style> td {white-space:nowrap;border:0.5pt solid #dee0e3;font-size:10pt;font-style:normal;font-weight:normal;vertical-align:middle;word-break:normal;word-wrap:normal;}</style>

| Function                        | Description                                                                                          | Input                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Return                                                                                                                                |
|---------------------------------|--------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|
| **Initialize**                  |                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                        |
| `client.InitRobot()`            | Initialize robot and scene                                                                             | `robot_cfg`: config file path (in 'robot_cfg' folder)<br>`scene_usd`: USD file path<br>`init_position([x,y,z])`<br>`init_rotation([x,y,z])` |                                                                                                                                        |
| **Controller**                  |                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                        |
| `client.moveto()`               | Move robot's end effector to target pose                                                               | `target_position([x,y,z])`<br>`target_quaternion([w,x,y,z])`<br>`is_backend`:<br>1. `True`: straight-line EE movement<br>2. `False`: EE movement with collision avoidance<br>`ee_interpolation`:<br>1. `True`: ruckig interpolation<br>2. `False`: target point interpolation<br>`distance_frame`: interpolation density |                                                                                                                                        |
| `client.set_joint_positions()`  | Move robot joints to target joint state                                                                | `target_joint_position`: [N], rad/s<br>`joint_indices`: [N], indices<br>`is_trajectory`:<br>1. `True`: apply joint action<br>2. `False`: set joint state |                                                                                                                                        |
| `client.set_gripper_state()`    | Set gripper state (open or close)                                                                      | `gripper_command`: "open" or "close"<br>`is_right`(bool)<br>`opened_width`(float) |                                                                                                                                        |
| `client.SetTrajectoryList()`    | Set EE pose list trajectory                                                                            | `trajectory_list(list)`: [[position(xyz), rotation(wxyz)]] |                                                                                                                                        |
| **Object**                      |                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                        |
| `client.add_object()`           | Add a USD object to the scene                                                                          | `usd_path(str)`: relative asset path based on `SIM_ASSETS`<br>`prim_path(str)`: object prim path in scene e.g. "/World/Object/obj_01"<br>`label_name(str)`: object label for semantic segmentation<br>`target_position([x,y,z])`<br>`target_quaternion([w,x,y,z])`<br>`target_scale([x,y,z])`<br>`mass`:[kg]<br>`add_particle(bool)`: add fluid particles e.g. water in kettle<br>`particle_size([x,y,z])`<br>`particle_position([x,y,z])` |                                                                                                                                        |
| `client.SetMaterial()`          | Set material of XFormPrim                                                                              | `material_info(list)`:<br>e.g.<br>`material_info = [{"object_prim": "/obj1", "material_name": "wood", "material_path": "materials/wood"}]` |                                                                                                                                        |
| `client.SetLight()`             | Set stage lighting info                                                                                | `light_info(list)`:<br>e.g.<br>`light_info = [{"light_type": "Distant", "light_prim": "/World/DistantLight", "light_temperature": 2000, "light_intensity": 1000, "rotation": [1,0.5,0.5,0.5], "texture": ""}]` |                                                                                                                                        |
| `client.SetObjectPose()`        | Set rigid or articulated object pose in a single physics step                                          | `object_info(list)`:<br>e.g.<br>`object_info = [{"prim_path": "/World/obj1", "position": [x,y,z], "rotation": [w,x,y,z]}]`<br>`object_joint_info(list)`: articulated object joint info |                                                                                                                                        |
| **Sensor**                      |                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                        |
| `client.AddCamera()`            | Add a camera to the stage                                                                              | `camera_prim(str)`: camera prim path<br>`camera_position(xyz)`: camera position<br>`camera_rotation(wxyz)`: camera rotation<br>`Width`, `Height`, `focus_length`, `horizontal_aperture`, `vertical_aperture` for camera intrinsics:<br>`fx = Width * focus_length / horizontal_aperture`<br>`fy = Height * focus_length / vertical_aperture`<br>`is_local(bool)`: local or world pose |                                                                                                                                        |
| `client.capture_frame()`        | Capture RGB/depth frame                                                                                | `camera_prim(str)`: camera prim path | `response.color_image.data`<br>`response.depth_image.data` |
| `client.capture_semantic_frame()` | Capture semantic frame                                                                               | `camera_prim_path(str)`: camera prim path | `response.semantic_mask.data` |
| **Observation**                 |                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                        |
| `client.get_observation()`      | Get camera/joint/tf data in a single frame                                                             | `data_keys(dict)`:<br>e.g.<br>`data_keys = {'camera': {'camera_prim_list': ["/camera"], 'render_depth': True, 'render_semantic': True}, 'pose': ["/object1"], 'joint_position': True, 'gripper': True}` | `observation = {"camera": camera_datas, "joint": joint_datas, "pose": object_datas, "gripper": gripper_datas}` |
| `client.GetIKStatus()`          | Inverse kinematics calculation                                                                         | `target_poses(list)`: [{"position":xyz, "rotation":wxyz}]<br>`is_right(bool)`: arm type<br>`ObsAvoid(bool)`: compute IK with collision avoidance | `"status"([bool])`: IK success<br>`"Jacobian"([double])`: Jacobian score of IK joint state<br>`"joint_positions"([list])`: IK joint positions<br>`"joint_names"([list])`: IK joint names |
| `client.GetEEPose()`            | Get end effector world pose                                                                            | `is_right(bool)`: select arm type | `state.ee_pose.position`<br>`state.ee_pose.rpy` |
| `client.get_object_pose()`      | Get object's world pose                                                                                | `prim_path(str)`: object prim path | `object_pose.position`<br>`object_pose.rpy` |
| `client.get_joint_positions()`  | Get robot's current joint positions                                                                    | | `result.states.name`<br>`result.states.position` |
| **Curobo Features**             |                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                        |
| `client.AttachObj()`            | Attach passive object                                                                                  | `prim_paths[list[str]]`: attached object prim paths |                                                                                                                                        |
| `client.DetachObj()`            | Detach all objects                                                                                     | |                                                                                                                                        |
| **Recording Settings**          |                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                        |
| `client.start_recording()`      | Record episode                                                                                         | `data_keys(dict)`: recording settings<br>e.g.<br>`data_keys = {'camera': {'camera_prim_list': ["/World/base_link/Head_Camera"], 'render_depth': True, 'render_semantic': True}, 'pose': ["/World/obj1"], 'joint_position': True, 'gripper': True}`<br>`fps(int)`: recording FPS<br>`task_name(str)` |                                                                                                                                        |
| `client.stop_recording()`       | Stop recording                                                                                         | |                                                                                                                                        |
| `client.reset()`                | Reset scene, robot, and objects                                                                        | |                                                                                                                                        |
| `client.Exit()`                 | Exit application                                                                                       | |                                                                                                                                        |

