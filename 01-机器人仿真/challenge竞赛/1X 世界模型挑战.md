# 1X 世界模型挑战

视频生成的进展可能很快使得在完全学习的世界模型中评估机器人策略成为可能。一个端到端学习的模拟器，可以模拟数百万个机器人环境，将极大加速通用机器人技术的进展，并为扩展数据和计算提供有用的信号。

为了加速机器人学习模拟器的进展，我们宣布了1X世界模型挑战，任务是预测[EVE Android](https://www.1x.tech/androids/eve)的未来第一人称观察。我们提供了超过100小时的矢量量化图像令牌和来自操作EVE时收集的原始动作数据，基线世界模型（GENIE风格）和一个帧级MAGVIT2自编码器，将图像压缩为16x16的令牌，并将其解码回图像。

我们希望这个数据集能为想要在人体环境中进行实验的机器人研究人员提供帮助。一个足够强大的世界模型将允许任何人访问“神经模拟的EVE”。评估挑战是最终目标，我们为中间目标提供现金奖励，如良好拟合数据（压缩挑战）和生成逼真视频（采样挑战）。

[Huggingface上的数据集](https://huggingface.co/datasets/1x-technologies/worldmodel)

[加入Discord](https://discord.gg/kk2HmvrQsN)

[第一阶段博客](https://www.1x.tech/discover/1x-world-model)，[第二阶段博客](https://www.1x.tech/discover/1x-world-model-sampling-challenge)

敬请期待1X世界模型挑战第二阶段的更新！

|                                                                               |                                                                               |                                                                               |                                                                               |                                                                               |                                                                                |                                                                               |                                                                               |
| ----------------------------------------------------------------------------- | ----------------------------------------------------------------------------- | ----------------------------------------------------------------------------- | ----------------------------------------------------------------------------- | ----------------------------------------------------------------------------- | ------------------------------------------------------------------------------ | ----------------------------------------------------------------------------- | ----------------------------------------------------------------------------- |
| ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset700100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset225100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset775100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset875100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset475100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset725100.gif)  | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset525100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset100.gif)    |
| ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset925100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset975100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset625100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset675100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset400100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset175100.gif)  | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset850100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset100100.gif) |
| ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset125100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset375100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset275100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset800100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset600100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset1000100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset450100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset50100.gif)  |
| ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset250100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset150100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset825100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset950100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset25100.gif)  | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset750100.gif)  | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset650100.gif) | ![til](https://7mlcen.aitianhu6.top/c/assets/v1.0/generated_offset300100.gif) |

## 挑战

每个示例是一系列16帧第一人称图像，来自机器人，帧速率为2Hz（共8秒），你的目标是给定先前的图像，预测下一张图像。

- **压缩挑战（$10k奖金）**：预测下一张图像中令牌的离散分布。
  - 标准：在我们的私人测试集上，首先实现一个**[时序教师强制](https://7mlcen.aitianhu6.top/c/67ccfd05-5854-800c-951e-b434719328e4#metric-details)损失低于8.0**。
- **采样挑战（$10k奖金）**：未来预测方法不一定局限于预测下一个logit。例如，你可以使用GANs、扩散模型和MaskGIT等方法生成未来图像。标准将稍后发布。
- **评估挑战（即将来临）**：给定一组N个策略，$\pi_1, \pi_2, ... \pi_N$，其中每个策略$\pi_i(a_t|z_t)$根据图像令牌预测动作令牌，能否在“世界模型”$p(z_{t+1}|z_t, a_t)$中评估所有策略，并告诉我们哪些策略排名最好？

这些挑战在很大程度上受到[commavq压缩挑战](https://github.com/commaai/commavq)的启发。请阅读[附加挑战细节](https://7mlcen.aitianhu6.top/c/67ccfd05-5854-800c-951e-b434719328e4#additional-challenge-details)。

## 开始

我们要求使用`Python 3.10`或更高版本。此代码已在`Python 3.10.12`上测试通过。

```
# 安装依赖并下载数据
./build.sh 

# 激活Python环境
source venv/bin/activate
```



```
conda create -n your_environment_name python=3.10.12
conda activate your_environment_name

# Install dependencies
conda install -c conda-forge accelerate=0.30.1 torchvision=0.18.0 lpips=0.1.4 matplotlib tqdm wandb xformers=0.0.26.post1 wheel packaging ninja einops

# For transformers and torch (ensure compatibility with your CUDA version)
pip install transformers==4.41.0 lightning>2.3.1 git+https://github.com/janEbert/mup.git@fsdp-fix

pip install triton
FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE python -m pip install flash-attn==2.5.8 --no-build-isolation
huggingface-cli download 1x-technologies/worldmodel --repo-type dataset --local-dir data
```

需要

export XFORMERS_FORCE_DISABLE_TRITON=1

## GENIE

此仓库提供了[Genie: Generative Interactive Environments](https://arxiv.org/abs/2402.15391)中描述的时空变换器和MaskGIT采样器的实现。请注意，这个实现仅在视频序列上训练，而不是动作（尽管通过添加嵌入非常简单就可以加入动作）。

```
# 训练GENIE模型
python train.py --genie_config genie/configs/magvit_n32_h8_d256.json --output_dir data/genie_model --max_eval_steps 10

# 从训练好的模型生成帧
python genie/generate.py --checkpoint_dir data/genie_model/final_checkpt

# 可视化生成的帧
python visualize.py --token_dir data/genie_generated

# 评估训练好的模型
python genie/evaluate.py --checkpoint_dir data/genie_model/final_checkpt
```

### 1X GENIE 基线

我们提供了两个预训练的GENIE模型，链接在[排行榜](https://7mlcen.aitianhu6.top/c/67ccfd05-5854-800c-951e-b434719328e4#leaderboard)中。

```
# 生成和可视化
output_dir='data/genie_baseline_generated'
for i in {0..240..10}; do
    python genie/generate.py --checkpoint_dir 1x-technologies/GENIE_138M \
        --output_dir $output_dir --example_ind $i --maskgit_steps 2 --temperature 0
    python visualize.py --token_dir $output_dir
    mv $output_dir/generated_offset0.gif $output_dir/example_$i.gif
    mv $output_dir/generated_comic_offset0.png $output_dir/example_$i.png
done

# 评估
python genie/evaluate.py --checkpoint_dir 1x-technologies/GENIE_138M --maskgit_steps 2
```

## 数据描述

[请查看Huggingface上的数据集卡片](https://huggingface.co/datasets/1x-technologies/worldmodel)。

训练数据集存储在`data/train_v1.1`目录中。

## 参与挑战：

请先阅读[附加挑战细节](https://7mlcen.aitianhu6.top/c/67ccfd05-5854-800c-951e-b434719328e4#additional-challenge-details)以了解规则。

发送源代码+构建脚本+一些关于你方法的信息至[challenge@1x.tech](mailto:challenge@1x.tech)。我们将在我们持有的数据集上评估你的提交，并通过邮件回复你结果。

请向我们发送以下内容：

- 你选择的用户名（可以是你的真实姓名或化名，将与邮件一对一绑定）
- 作为.zip文件提交源代码
- 训练模型时大约使用的FLOP数量
- 训练模型时使用的任何外部数据
- 你在提供的验证集上的评估性能（这样我们大致知道你对模型的期望）

在手动审查你的代码后，我们在22.04 + CUDA 12.3沙盒环境中运行评估，命令如下：

```
./build.sh # 安装所需的依赖和模型权重
./evaluate.py --val_data_dir <PATH-TO-HELD-OUT-DATA>  # 在保留的数据集上运行你的模型
```

## 附加挑战细节

1. 我们已提供`magvit2.ckpt`，这是一个[MAGVIT2](https://github.com/TencentARC/Open-MAGVIT2)编码器/解码器的权重。该编码器允许你对外部数据进行令牌化，以帮助提高度量。
2. 与LLM不同，损失度量不标准，因为图像令牌的词汇大小，在v1.0版本发布时（2024年7月8日）已更改。我们不再对具有2^18个类的logits计算交叉熵损失，而是对2x 2^9个类的预测计算交叉熵，并将其加总起来。这样做的原因是大词汇大小（2^18）使得存储一个形状为`(B, 2^18, T, 16, 16)`的logit张量在内存中非常占用空间。因此，压缩挑战会考虑具有以下形式的模型族的因式分解的PMF：p(x1, x2) = p(x1)p(x2)。对于采样和评估挑战，因式分解的PMF是必要标准。
3. 对于压缩挑战，我们故意选择在训练时固定因式分解的分布p(x1, x2) = p(x1)p(x2)来评估保留数据。尽管无因式分解模型（如p(x1, x2) = f(x1, x2)）应该通过利用x1和x2的协方差的非对角项在测试数据上获得更低的交叉熵，但我们希望鼓励那些在保持因式分解固定的情况下获得更低损失的解决方案。
4. 对于压缩挑战，提交仅可以使用*当前提示帧之前*的动作。提交可以自回归预测后续动作以提高性能，但这些动作将不会与提示一起提供。
5. 朴素的最近邻检索+从训练集中寻找下一帧将能在开发验证集上取得相当好的损失和采样结果，因为在训练集里有类似的序列。然而，我们明确禁止这种解决方案（私有测试集会对这种解决方案进行惩罚）。
6. 如果违反挑战精神，我们将无法奖励来自美国制裁国家的个人。我们保留在挑战中不授予奖金的权利。

### 度量标准细节

评估的情景有不同的标准，具体取决于模型所接收的真实上下文的程度。
按照上下文的递减顺序，这些情景为：

- **完全自回归**：模型接收预定数量的真实帧，并自回归地预测所有剩余帧。
- **时序教师强制**：模型接收当前帧之前的所有真实帧，并自回归地预测当前帧的所有令牌。
- **完全教师强制**：模型接收当前帧之前的所有真实令牌，包括当前帧中的令牌。仅适用于因果LM。

例如，考虑预测视频的最终令牌，位于第15帧的右下角。
在每种情景中，模型接收的上下文如下：

- 完全自回归：前$t$个16x16令牌是前$t$帧的真实令牌，其余令牌是自回归生成的，$0 < t < 15$为预定的提示帧数量。
- 时序教师强制：前15个16x16令牌是前15帧的真实令牌，其余令牌是自回归生成的。
- 完全教师强制：所有前（16x16x16 - 1）个令牌是真实令牌。

压缩挑战使用“时序教师强制”情景。

## 排行榜

这些是`data/val_v1.1`上的评估结果。

## 帮助我们改进挑战！

除了世界模型挑战之外，我们还希望使挑战和数据集对*你的*研究问题更加有用。想要更多与人类互动的数据吗？更多像是小心搬运热咖啡杯等安全关键任务？更多精巧的工具使用？机器人与其他机器人合作？机器人在镜子中给自己穿衣？把1X当作获取高质量类人数据的操作团队，处理各种极其多样化的场景。

请将你的数据需求（以及你认为这些数据为什么重要）通过邮件发送至[challenge@1x.tech](mailto:challenge@1x.tech)，我们将尽力将其纳入未来的数据发布。你还可以在[Discord](https://discord.gg/UMnzbTkw)上与社区讨论你的数据需求。

我们也欢迎捐助者来帮助我们增加奖金。

## 引用

如果你在工作中使用了此软件或数据集，请使用Github上的“引用此仓库”按钮进行引用。

## 更新日志

- v1.1 - 发布压缩挑战标准；从数据集中移除暂停和不连续视频；更高的图像裁剪。
- v1.0 - 更高效的MAGVIT2令牌化器，使用16x16 (C=2^18) 映射到256x256的图像，并提供原始动作数据。
- v0.0.1 - 初始挑战发布，使用20x20 (C=1000) 图像令牌化器，映射到160x160的图像。
